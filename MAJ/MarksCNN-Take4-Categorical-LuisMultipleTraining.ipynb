{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T04:02:51.984661Z",
     "start_time": "2019-07-29T04:02:51.976385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Imports done.  Using Tensorflow version: 1.13.1, and Keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, argparse, random, pickle, cv2, fnmatch, PIL, math, signal\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('[INFO] Imports done.  Using Tensorflow version: {}, and Keras version: {}'.format(tf.__version__, k.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T04:02:54.532203Z",
     "start_time": "2019-07-29T04:02:54.525897Z"
    }
   },
   "outputs": [],
   "source": [
    "config={}\n",
    "\n",
    "config['note'] = '-CatAllData'\n",
    "\n",
    "config['new_directory'] = ''\n",
    "\n",
    "config['do_dataprep'] = False\n",
    "\n",
    "config['source'] = os.path.join(os.getcwd(), 'source_data')\n",
    "config['home_dir'] = os.path.join(os.getcwd(), 'data_cat_split')\n",
    "\n",
    "config['train_dir'] = os.path.join(config['home_dir'], 'train')\n",
    "config['test_dir'] = os.path.join(config['home_dir'], 'test')\n",
    "config['val_dir'] = os.path.join(config['home_dir'], 'validation')\n",
    "\n",
    "config['cat_list'] = ['{}.0'.format(i) for i in range(0, 16)]\n",
    "\n",
    "config['val_split'] = 0.1\n",
    "config['test_split'] = 0.1\n",
    "\n",
    "config['target_size'] = (320, 240)\n",
    "\n",
    "config['train_batch'] = 100\n",
    "config['test_batch'] = 100\n",
    "config['val_batch'] = 100\n",
    "\n",
    "config['epochs'] = 1\n",
    "\n",
    "config['set_limit'] = False\n",
    "config['train_limit'] = 100\n",
    "config['test_val_limit'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T04:02:55.566020Z",
     "start_time": "2019-07-29T04:02:55.562262Z"
    }
   },
   "outputs": [],
   "source": [
    "def working_dir(): \n",
    "    config['new_directory'] = os.path.join(os.getcwd()\n",
    "                                       , 'marks-runs'\n",
    "                                       , '{}, {}'.format(datetime.datetime.now().strftime(\"%y%m%d%H%M\"), config['note']))\n",
    "\n",
    "    if os.path.exists(config['new_directory']):\n",
    "            shutil.rmtree(config['new_directory'])\n",
    "\n",
    "    os.mkdir(config['new_directory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T04:02:57.280169Z",
     "start_time": "2019-07-29T04:02:57.272950Z"
    }
   },
   "outputs": [],
   "source": [
    "def split(SOURCE, TRAINING, TESTING, VALIDATION):\n",
    "    \n",
    "    print('[INFO] Splitting: {}'.format(SOURCE))\n",
    "\n",
    "    files = []\n",
    "    \n",
    "    for file in os.listdir(SOURCE): \n",
    "        files.append(file)\n",
    "\n",
    "    training_length = int(len(files) * (1-config['val_split']-config['test_split']))\n",
    "    testing_length = int(len(files) * config['test_split'])\n",
    "    validation_length = int(len(files) * config['val_split'])\n",
    "\n",
    "    shuffled_set = random.sample(files, len(files))\n",
    "\n",
    "    training_set = shuffled_set[0:training_length]\n",
    "    testing_set = shuffled_set[training_length:training_length+testing_length]\n",
    "    validation_set = shuffled_set[training_length+testing_length:]\n",
    "\n",
    "    for filename in tqdm(training_set):\n",
    "        this_file = os.path.join(SOURCE, filename)\n",
    "        destination = os.path.join(TRAINING, filename)\n",
    "        shutil.copyfile(this_file, destination)\n",
    "\n",
    "    for filename in tqdm(testing_set):\n",
    "        this_file = os.path.join(SOURCE, filename)\n",
    "        destination = os.path.join(TESTING, filename)\n",
    "        shutil.copyfile(this_file, destination)\n",
    "\n",
    "    for filename in tqdm(validation_set):\n",
    "        this_file = os.path.join(SOURCE, filename)\n",
    "        destination = os.path.join(VALIDATION, filename)\n",
    "        shutil.copyfile(this_file, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T04:02:57.809735Z",
     "start_time": "2019-07-29T04:02:57.804398Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_clean_dirs(): \n",
    "    new_dirs = [config['train_dir'], config['test_dir'], config['val_dir']]\n",
    "    if os.path.exists(config['home_dir']):\n",
    "        shutil.rmtree(config['home_dir'])\n",
    "    os.mkdir(config['home_dir'])\n",
    "        \n",
    "    for value in new_dirs: \n",
    "        parent_folder = os.path.join(config['home_dir'], value)\n",
    "        if not os.path.exists(parent_folder):\n",
    "            os.mkdir(parent_folder)\n",
    "            for cat in config['cat_list']: \n",
    "                temp = cat.split('.')[0]\n",
    "                if int(temp) <= 9: \n",
    "                    temp2 = '0{}'.format(temp)\n",
    "                else: \n",
    "                    temp2 = temp\n",
    "                print('class{}'.format(temp2))\n",
    "                child_folder = os.path.join(parent_folder, temp2)\n",
    "                if not os.path.exists(child_folder):\n",
    "                    os.mkdir(child_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T04:02:58.699658Z",
     "start_time": "2019-07-29T04:02:58.679420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class00\n",
      "class01\n",
      "class02\n",
      "class03\n",
      "class04\n",
      "class05\n",
      "class06\n",
      "class07\n",
      "class08\n",
      "class09\n",
      "class10\n",
      "class11\n",
      "class12\n",
      "class13\n",
      "class14\n",
      "class15\n",
      "class00\n",
      "class01\n",
      "class02\n",
      "class03\n",
      "class04\n",
      "class05\n",
      "class06\n",
      "class07\n",
      "class08\n",
      "class09\n",
      "class10\n",
      "class11\n",
      "class12\n",
      "class13\n",
      "class14\n",
      "class15\n",
      "class00\n",
      "class01\n",
      "class02\n",
      "class03\n",
      "class04\n",
      "class05\n",
      "class06\n",
      "class07\n",
      "class08\n",
      "class09\n",
      "class10\n",
      "class11\n",
      "class12\n",
      "class13\n",
      "class14\n",
      "class15\n"
     ]
    }
   ],
   "source": [
    "create_clean_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T04:03:00.061405Z",
     "start_time": "2019-07-29T04:03:00.057421Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_dir_size(): \n",
    "    # create_clean_dirs()\n",
    "    new_dirs = [config['train_dir'], config['test_dir'], config['val_dir']]\n",
    "    for directory in new_dirs: \n",
    "        for sub_dir in os.listdir(directory): \n",
    "            file_counter = sum([len(files) for r, d, files in os.walk(os.path.join(config['train_dir'], directory, sub_dir))])\n",
    "            print('{}/{}: has {} files'.format(directory, sub_dir, file_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T04:03:00.318500Z",
     "start_time": "2019-07-29T04:03:00.314763Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config['do_dataprep']: \n",
    "    \n",
    "    # Part 1: Make Directories\n",
    "    \n",
    "    create_clean_dirs()\n",
    "    check_dir_size()\n",
    "    \n",
    "    # Part 2: Loop on all directories and call the split function\n",
    "    \n",
    "    for cat in config['cat_list']:\n",
    "        split(\n",
    "            SOURCE = os.path.join(config['source'], cat)\n",
    "            , TRAINING = os.path.join(config['train_dir'], cat) \n",
    "            , TESTING = os.path.join(config['test_dir'], cat) \n",
    "            , VALIDATION = os.path.join(config['val_dir'], cat) \n",
    "        )\n",
    "        \n",
    "    check_dir_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T04:03:01.913612Z",
     "start_time": "2019-07-29T04:03:01.697453Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train_pain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f85c0f52fa2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_limiting_dirs = [\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_pain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_nopain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     ]\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m test_val_limiting_dirs = [\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_pain'"
     ]
    }
   ],
   "source": [
    "train_limiting_dirs = [\n",
    "        config['train_pain'], config['train_nopain']\n",
    "    ]\n",
    "\n",
    "test_val_limiting_dirs = [\n",
    "        config['test_pain'], config['test_nopain']\n",
    "        , config['val_pain'], config['val_nopain']\n",
    "    ]\n",
    "\n",
    "if config['set_limit']: \n",
    "    print('Before')\n",
    "    for d in train_limiting_dirs: \n",
    "        print('{}: {}'.format(d, len(os.listdir(d))))\n",
    "    for d in test_val_limiting_dirs: \n",
    "        print('{}: {}'.format(d, len(os.listdir(d))))\n",
    "        \n",
    "    for d in train_limiting_dirs: \n",
    "        counter = 1\n",
    "        for filename in os.listdir(d): \n",
    "            if counter > config['train_limit']:\n",
    "                os.remove(os.path.join(d, filename))\n",
    "            counter = counter +1\n",
    "    \n",
    "    for d in test_val_limiting_dirs: \n",
    "        counter = 1\n",
    "        for filename in os.listdir(d): \n",
    "            if counter > config['test_val_limit']:\n",
    "                os.remove(os.path.join(d, filename))\n",
    "            counter = counter +1\n",
    "\n",
    "    print('\\nAfter')\n",
    "    for d in train_limiting_dirs: \n",
    "        print('{}: {}'.format(d, len(os.listdir(d))))\n",
    "    for d in test_val_limiting_dirs: \n",
    "        print('{}: {}'.format(d, len(os.listdir(d))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T04:03:22.585848Z",
     "start_time": "2019-07-29T04:03:22.476066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "config['train_count'] = sum([len(files) for r, d, files in os.walk(config['train_dir'])])\n",
    "config['train_steps'] = math.ceil(config['train_count']/config['train_batch'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0\n",
    "#     , rotation_range=40\n",
    "#     , width_shift_range=0.2\n",
    "#     , height_shift_range=0.2\n",
    "#     , shear_range=0.2\n",
    "#     , zoom_range=0.2\n",
    "#     , horizontal_flip=True\n",
    "#     , fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    config['train_dir']\n",
    "    , target_size = config['target_size'] \n",
    "    , batch_size = config['train_batch']\n",
    "    , class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:28:34.002431Z",
     "start_time": "2019-07-17T12:28:33.788544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4854 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "config['val_count'] = sum([len(files) for r, d, files in os.walk(config['val_dir'])])\n",
    "config['val_steps'] = math.ceil(config['val_count']/config['val_batch'])\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    config['val_dir']  # This is the source directory for training images\n",
    "    , target_size = config['target_size']  # All images will be resized to 150x150 for compressing\n",
    "    , batch_size = config['val_batch']\n",
    "    , class_mode = 'categorical' # Since we use binary_crossentropy loss, we need binary labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:28:34.220138Z",
     "start_time": "2019-07-17T12:28:34.004565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4833 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "config['test_count'] = sum([len(files) for r, d, files in os.walk(config['test_dir'])])\n",
    "config['test_steps'] = math.ceil(config['test_count']/config['test_batch'])\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    config['test_dir']  # This is the source directory for training images\n",
    "    , target_size = config['target_size']  # All images will be resized to 150x150 for compressing\n",
    "    , batch_size = config['test_batch']\n",
    "    , class_mode = 'categorical' # Since we use binary_crossentropy loss, we need binary labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:28:35.443315Z",
     "start_time": "2019-07-17T12:28:35.439267Z"
    }
   },
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('categorical_accuracy')>0.99):\n",
    "            print('\\nReached 99% accuracy which is satisfactory so stopped training!')\n",
    "            self.model.stop_training = True\n",
    "        if(logs.get('categorical_accuracy')<0.4):\n",
    "            print('\\nUnable to go over 40% accuracy, so cancelling run!')\n",
    "            self.model.stop_training = True\n",
    "\n",
    "markscallbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:28:35.919233Z",
     "start_time": "2019-07-17T12:28:35.911374Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_and_compile_model(num_layers, window_size): \n",
    "    \n",
    "    cnn_config = [k.layers.Conv2D(16, (window_size,window_size), activation='relu', input_shape=(*config['target_size'], 3), padding = 'same', name='mh-conv-1')\n",
    "                  , k.layers.MaxPooling2D(2, 2, name='mh-maxpool-1')]\n",
    "    \n",
    "    for i in range(2,num_layers+1,1):\n",
    "        cnn_config.append([k.layers.Conv2D(32, (window_size, window_size), activation='relu', name='mh-conv-2'),\n",
    "                           k.layers.MaxPooling2D(2,2, name='mh-maxpool-2')])\n",
    "        \n",
    "            , k.layers.Conv2D(64, (3,3), activation='relu', name='mh-conv-3')\n",
    "            , k.layers.MaxPooling2D(2,2, name='mh-maxpool-3')\n",
    "            , k.layers.Conv2D(128, (3,3), activation='relu', name='mh-conv-4')\n",
    "            , k.layers.MaxPooling2D(2,2, name='mh-maxpool-4')\n",
    "            , k.layers.Conv2D(256, (3,3), activation='relu', name='mh-conv-5')\n",
    "            , k.layers.MaxPooling2D(2,2, name='mh-maxpool-5')\n",
    "            , k.layers.Flatten(name='mh-flatten-1')\n",
    "            , k.layers.Dropout(0.5)\n",
    "            , k.layers.Dense(1024, activation='relu', name='mh-dense-1')\n",
    "            , k.layers.Dropout(0.2)\n",
    "            , k.layers.Dense(128, activation='relu', name='mh-dense-2')\n",
    "            , k.layers.Dropout(0.2)\n",
    "            , k.layers.Dense(16, activation='softmax', name='mh-dense-output')\n",
    "            ]\n",
    "    \n",
    "    \n",
    "    model = k.models.Sequential(cnn_config)\n",
    "    model.compile(\n",
    "        optimizer=Adam()\n",
    "        , loss='categorical_crossentropy'\n",
    "        , metrics=['categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:28:37.530670Z",
     "start_time": "2019-07-17T12:28:37.148782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mh-conv-1 (Conv2D)           (None, 320, 240, 16)      448       \n",
      "_________________________________________________________________\n",
      "mh-maxpool-1 (MaxPooling2D)  (None, 160, 120, 16)      0         \n",
      "_________________________________________________________________\n",
      "mh-conv-2 (Conv2D)           (None, 158, 118, 32)      4640      \n",
      "_________________________________________________________________\n",
      "mh-maxpool-2 (MaxPooling2D)  (None, 79, 59, 32)        0         \n",
      "_________________________________________________________________\n",
      "mh-conv-3 (Conv2D)           (None, 77, 57, 64)        18496     \n",
      "_________________________________________________________________\n",
      "mh-maxpool-3 (MaxPooling2D)  (None, 38, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "mh-conv-4 (Conv2D)           (None, 36, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "mh-maxpool-4 (MaxPooling2D)  (None, 18, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "mh-conv-5 (Conv2D)           (None, 16, 11, 256)       295168    \n",
      "_________________________________________________________________\n",
      "mh-maxpool-5 (MaxPooling2D)  (None, 8, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "mh-flatten-1 (Flatten)       (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "mh-dense-1 (Dense)           (None, 1024)              10486784  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "mh-dense-2 (Dense)           (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "mh-dense-output (Dense)      (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 11,012,656\n",
      "Trainable params: 11,012,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for num_layers in range(2,5,1):\n",
    "    for w_size in range(3,20,1):\n",
    "        model = create_and_compile_model(num_layers,w_size)\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:35:17.213570Z",
     "start_time": "2019-07-17T12:28:39.229076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 56s 1s/step - loss: 0.6178 - categorical_accuracy: 0.8272\n",
      "388/388 [==============================] - 396s 1s/step - loss: 0.7564 - categorical_accuracy: 0.8253 - val_loss: 0.6178 - val_categorical_accuracy: 0.8272\n",
      "\n",
      "[INFO] Training Complete\n"
     ]
    }
   ],
   "source": [
    "working_dir()\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator\n",
    "    , steps_per_epoch = config['train_steps']\n",
    "    , epochs = config['epochs']\n",
    "    , validation_data = val_generator\n",
    "    , validation_steps = config['val_steps']\n",
    "    , callbacks=[\n",
    "        markscallbacks\n",
    "        , EarlyStopping(monitor='val_categorical_accuracy', patience=10)\n",
    "        , ModelCheckpoint(filepath=os.path.join(config['new_directory'], 'model.h5'), monitor='val_categorical_accuracy', save_best_only=True)\n",
    "        , ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('\\n[INFO] Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:36:19.986734Z",
     "start_time": "2019-07-17T12:35:25.452248Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted = model.predict_generator(test_generator, steps=config['test_steps'])\n",
    "labels = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:39:12.003855Z",
     "start_time": "2019-07-17T12:39:12.000365Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Number of classes\n",
    "n_classes = 16\n",
    "\n",
    "# Plot linewidth.\n",
    "lw = 2\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:39:12.775451Z",
     "start_time": "2019-07-17T12:39:12.762951Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-77985c18aa32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(labels[:, i], predicted[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:42:10.547983Z",
     "start_time": "2019-07-17T12:42:10.544391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:37:55.463152Z",
     "start_time": "2019-07-17T12:37:55.443208Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ad18bf58536e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels.ravel(), predicted.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:17:25.781598Z",
     "start_time": "2019-07-17T12:14:05.304Z"
    }
   },
   "outputs": [],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(labels, predicted)\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "\n",
    "plt.subplot(1, 2 ,1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "# plt.show()\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(hist['epoch'], hist['binary_accuracy'], label='Train Binary Accuracy')\n",
    "plt.plot(hist['epoch'], hist['val_binary_accuracy'], label = 'Val Binary Accuracy')\n",
    "plt.title('Accuracy vs Training')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(os.path.join(config['new_directory'], 'result plots.png'))\n",
    "\n",
    "hist.to_csv(os.path.join(config['new_directory'], 'run_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:17:25.782821Z",
     "start_time": "2019-07-17T12:14:06.177Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Completed running Notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T12:17:25.783912Z",
     "start_time": "2019-07-17T12:14:07.123Z"
    }
   },
   "outputs": [],
   "source": [
    "os.kill(os.getpid(), signal.SIGKILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
